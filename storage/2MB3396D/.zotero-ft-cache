See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/354838651
Forecasting Unemployment in the Euro‐Area with Machine Learning
Article in Journal of Forecasting · September 2021
DOI: 10.1002/for.2824

CITATIONS
4
3 authors: Periklis Gogas Democritus University of Thrace 145 PUBLICATIONS 1,241 CITATIONS
SEE PROFILE
Emmanouil Sofianos Democritus University of Thrace 5 PUBLICATIONS 14 CITATIONS
SEE PROFILE

READS
458
Theophilos Papadimitriou Democritus University of Thrace 138 PUBLICATIONS 986 CITATIONS
SEE PROFILE

Some of the authors of this publication are also working on these related projects: Forecasting bank failures and private credit defaults View project Complex Networks: The Threshold - Minimum Dominating Set (T-MDS) View project

All content following this page was uploaded by Periklis Gogas on 29 September 2021.
The user has requested enhancement of the downloaded file.

Journal of Forecasting

Forecasting Unemployment in the Euro-Area with Machine Learning

Journal: Journal of Forecasting

Manuscript ID FOR-21-0042

Wiley - Manuscript type: Research Article

Date Submitted by the Author:

27-Jan-2021

Complete List of Authors: Sofianos, Emmanouil; Demokriteio Panepistemio Thrakes Gogas, Periklis; Demokriteio Panepistemio Thrakes Papadimitriou, Theophilos; Demokriteio Panepistemio Thrakes

Keywords:

Keywords: Unemployment Rate, Euro Area, Machine Learning, Forecasting

Abstract:

Unemployment has a direct impact on public finances and yields serious sociopolitical implications. This study aims to directionally forecast the euro-area unemployment rate. To the best of our knowledge, no other studies forecast the euro-area unemployment rate as a whole. The data set includes the unemployment rate and 36 explanatory variables, as suggested by theory and the relevant literature, spanning the period from 1998:4 to 2019:9 in monthly frequency. These variables are fed to three machine learning methodologies: Decision Trees (DT), Random Forests (RF), and Support Vector Machines (SVM), while an Elastic-Net Logistic Regression (Logit) model is used from the area of Econometrics. The results show that the optimal random forest model outperforms the other models by reaching a full-dataset forecasting accuracy of 88.5% and 85.4% on the out-of-sample.

For Peer Review

http://mc.manuscriptcentral.com/for

Page 1 of 28
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60

For Peer Review

Journal of Forecasting
Forecasting Unemployment in the Euro-Area with Machine Learning
Periklis Gogas, Theophilos Papadimitriou, Emmanouil Sofianos*
Department of Economics, Democritus University of Thrace, 69100 Komotini, Greece;
pgkogkas@econ.duth.gr (P.G.); papadimi@econ.duth.gr (T.P.) * Correspondence: esofiano@econ.duth.gr
Abstract
Unemployment has a direct impact on public finances and yields serious sociopolitical implications. This study aims to directionally forecast the euro-area unemployment rate. To the best of our knowledge, no other studies forecast the euroarea unemployment rate as a whole. The data set includes the unemployment rate and 36 explanatory variables, as suggested by theory and the relevant literature, spanning the period from 1998:4 to 2019:9 in monthly frequency. These variables are fed to three machine learning methodologies: Decision Trees (DT), Random Forests (RF), and Support Vector Machines (SVM), while an Elastic-Net Logistic Regression (Logit) model is used from the area of Econometrics. The results show that the optimal random forest model outperforms the other models by reaching a full-dataset forecasting accuracy of 88.5% and 85.4% on the out-of-sample.
JEL classification: C53; E24; E27 Keywords: Unemployment Rate; Euro Area; Decision Trees; Random Forest; SVM; Machine Learning; Forecasting;

http://mc.manuscriptcentral.com/for

Journal of Forecasting

Page 2 of 28

1

2

3 4

1. Introduction

5

The unemployment rate is an important macroeconomic variable. It significantly

6

7

impacts the individuals that remain out of the work force both financially and psychologically

8

9

as much as the households that they support. At the macroeconomic level, a high

10

11

unemployment rate has detrimental effects both on the society and the national economy. An

12

13

increasing unemployment rate reduces tax revenue (due to the loss of income by the

14

15

unemployed) and increases government spending in terms of unemployment benefits and the

16

17 18

associated re-training and placement programs. In countries where the central bank’s mandate

19 20

includes not only the preservation of monetary stability but also the goal of full employment,

For Peer Review

21

22

an increased unemployment rate may increase inflationary pressure. Therefore, being able to

23

24

timely forecast changes in the unemployment rate, is important for fiscal and monetary policy

25

26

makers in order to implement relevant policies.

27

28

The significance of unemployment rate forecasting was first acknowledged during the

29

30

1970s, when for the first time the problem of stagflation (i.e. the coexistence of high inflation

31

32

with high unemployment and slow economic growth) emerged (Brunner et al., 1980, Grubb et

33

34

al., 1982). Before the 1973 stagflation, economists relied in the traditional Phillips Curve that

35

36 37

assumed an inverse relationship between the unemployment and the inflation rate. Τhis

38 39

relation, eventually, proved to be a mere statistical stylized fact, valid only in the short-run

40

41

when monetary policy is unexpected and/or not credible (Muth, 1961).

42

43

Many statistical and econometric techniques have been employed in the voluminous

44

45

and rich literature of unemployment forecasting, most of them burdened with shortcomings,

46

47

ranging from high sensitivity to model specification, to extreme requirements with respect to

48

49

the data (Cook and Smalter Hall, 2017). Recently, economists turned their attention to models

50

51

free from these shortcomings from the area of Machine Learning.

52

53

In the last 20 years, the ARIMA and GARCH models were used extensively to

54

55

forecast the unemployment rate. Dobre and Alexandru (2008) used ARIMA in the case of

56

57 58

Romania and Floros (2005) used GARCH in the case of the UK. Mladenovic et al. (2017)

59

60

1 http://mc.manuscriptcentral.com/for

Page 3 of 28
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60

For Peer Review

Journal of Forecasting
used ARIMA models to forecast unemployment in the EU28. Kurita (2010) used ARFIMA models to forecast the unemployment of Japan.
Moreover, models using Google Index (job-search index) and data mining methods are applied to forecast the unemployment rate for the US (D’Amuri and Marcucci 2010, Xu, et al. 2013).
With respect to Machine Learning models, Sermpinis et al. (2014), use a hybrid genetic algorithm-support vector regression (GA-SVR) to forecast the US inflation and the unemployment. The forecasting performance of the GA-SVR model is tested against a random walk model, an autoregressive moving average model, a moving average convergence/divergence model, a multi-layer perceptron, a recurrent neural network, and a genetic programming algorithm. The GA-SVR outperforms the competition. Cook and Smalter Hall, (2017) created four different neural network architectures to forecast the US unemployment rate. Their results outperformed the SPF (Survey of Professional Forecasters), used as a benchmark. Similarly, Kreiner and Duca (2019) use Artificial Neural Networks (ANN) to forecast the US unemployment outperforming the SPF benchmark results. Stasinakis et al. (2014) forecast the US unemployment rate using radial basis function neural networks (RBFNN), Kalman filters and support vector regression. The models are tested against an ARMA, a STAR and three different ΑNN. Their results show that the RBFNN statistically outperforms all other models.
Despite the rich literature on forecasting the US unemployment rate, there is a limited number of papers forecasting unemployment in the eurozone. Most of the relevant studies focus their attention in the national level and to the case of specific countries. Dumisic et al. (2015) use double exponential smoothing and the Holt-Winters' methods to forecast the unemployment rate in Greece, Spain, Croatia, Italy and Portugal. The authors conclude that in all the analyzed countries, the double exponential smoothing method, is the most accurate with the exception of Italy where the Holt-Winters' method dominated. Barot (2004), assesses the accuracy of the Swedish domestic forecasters for GDP growth, CPI and unemployment. The best directional forecasting model for unemployment reached an accuracy of 75%.
2 http://mc.manuscriptcentral.com/for

Journal of Forecasting

Page 4 of 28

1

2

3

Claveria (2019), investigates the unemployment rate for eight European countries with

4

5

ARIMA models including as predictors both an indicator of unemployment, based on the

6

7 8

degree of agreement in consumer unemployment expectations, and a measure of disagreement

9 10

based on the dispersion of expectations. The forecasting models are tested on out-of-sample

11

12

data. According to the tests the degree of agreement in consumers’ expectations contains

13

14

useful information in predicting unemployment rates, especially for the detection of turning

15

16

points. Katris (2019), forecasts the unemployment rate in 22 European countries using the

17

18

FARIMA model with GARCH errors. The author considers the non-linearity of the data and

19

20

uses artificial neural networks, support vector regression and multivariate adaptive regression

For Peer Review

21

22

splines on his tests. FARIMA models are the optimal choice for the 1-step ahead forecasts,

23

24

while for the longer forecasting period (h=12) the neural network approaches achieves

25

26 27

comparable results with the FARIMA-based models.

28 29

In this paper we employ several Machine Learning models to directionally forecast

30

31

the euro-area unemployment rate as a whole. The models used are the Support Vector

32

33

Machines (SVM), the Decision Trees (DT), the Random Forests (RF) and the Elastic-Net

34

35

Logistic Regression (Logit) model. The innovations of our approach are: a) we focus our

36

37

study in the supranational level of the euro area as a whole and not to specific countries

38

39

within the E.U. and b) we directionally forecast the unemployment rate using several

40

41

classification models from the area of Machine Learning. To the best of our knowledge, no

42

43

other study attempts to forecast the direction of euro-area unemployment rate. Moreover, we

44

45

use the permutation Variable Importance Measure (VIM) in the RF methodology to identify

46

47 48

the significance of the predictors used in the model.

49

50

The paper is organized as follows: in Section 2 we will briefly discuss the

51

52

methodologies and the data while in Section 3 we describe our empirical results. Finally,

53

54

Section 4 concludes the paper.

55

56

57

58

59

60

3 http://mc.manuscriptcentral.com/for

Page 5 of 28
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60

Journal of Forecasting

2. Methodology and dataset

2.1. SVM
Support vector machines (SVM) is a set of methods for data classification and

regression based on the maximization of the interclass distance: the basic concept of the SVM

is to define the optimal1 linear separator that separates the data points into two classes.

Let’s consider a dataset of N m-sized vectors 𝐱𝑖∈Rm (𝑖=1, 2…, N). These vectors are

labelled according to the class they belong 𝑦𝑖 ∈ {−1, +1}.

If the two classes are linearly separable, we define a boundary as:

𝑓(𝐱𝑖) = 𝐰𝑻𝐱𝑖 - 𝑏 = 0.

( (1)

Subject to: 𝐰𝑻𝐱𝑖 - 𝑏 > 0 ∀𝑖: 𝑦𝑖 = +1, 𝐰𝑻𝐱𝑖 - 𝑏 < 0 ∀𝑖: 𝑦𝑖 = - 1,
w is the parameter vector, and b is the bias (Figure 1). So 𝑦𝑖𝑓(𝐱𝑖) > 0, ∀𝑖.

( (2)

For Peer Review

Figure 1. Hyperplane selection and support vectors. The SVs (represented with the pronounced black contour) define the margins that are represented with the dashed lines and outline the separating hyperplane represented by the continuous line.
1 optimal in the sense of the model generalization to unknown data
4 http://mc.manuscriptcentral.com/for

Journal of Forecasting

Page 6 of 28

1

2

3

4

5

In the linearly separable case, the separator (line in two dimensions, plane in three

6

7

dimensions, and hyperplane for higher dimensions spaces) is defined as the decision boundary

8

9

that classifies each data point to the correct class. A small subset of datapoints called support

10

11

vectors (SV) define the position of the separator. In Figure 1, the SV are represented with the

12

13

pronounced black contour, the margin lines (parallel lines to the optimal separator passing

14

15 16

from the SV) are represented by dashed lines, and the separator is represented by a continuous

17

18

line. The distance between the two margin lines is the distance between the two classes. The

19

20

goal of SVM is to identify the linear separator that maximizes the distance between the two

For Peer Review

21

22

classes.

23

24

In real-life phenomena, datasets are often contaminated with noise and may contain

25

26

outliers. These cases cannot be approached using the presented methodology. Cortes and

27

28

Vapnik (1995), introduced non-negative slack variables and a parameter C, describing the

29

30

desired tolerance to classification errors to treat these cases.

31

32

When the dataset is not linearly separable, the SVM model is paired with kernels: The

33

34 35

initial data space is projected through a kernel function into a space of higher dimensionality

36

37

(called feature space) where the dataset may be linearly separable. In Figure 2, we show a

38

39

dataset that is not linearly separable in the two-dimensional data space (left graph), denoted

40

41

by the red and blue circles. By projecting the dataset in a three-dimensional feature space

42

43

(right graph) using a kernel function, the linear separation is feasible.

44

45

46

47

48

49

50

51

52

53

54

55

56

57

58

59

60

5 http://mc.manuscriptcentral.com/for

Page 7 of 28
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60

For Peer Review

Journal of Forecasting
Figure 2. The non-separable two-class scenario in the data space (left) and the separable case in the feature space after the projection (right).
In this paper, we examine three kernels, the linear, the radial basis function (RBF) and the polynomial kernel. The linear kernel defines the separating hyperplane in the original dimensions of the data space, while the RBF and the polynomial project the initial dataset to a higher dimensional space. You may find a detailed analysis of the SVM methodology in Gogas et al. (2019). (Our implementation of SVM models is based on LIBSVM2, Chang and Lin., 2011).
2.2. Decision Trees
The decision trees are supervised machine learning methodologies for classification and regression. They are flowchart-like top-down structures of nodes and branches. Each node represents a splitting criterion (calculated with an impurity measure) and each branch the corresponding outcome. The top node is the root node representing the complete dataset; the other ones are called decision nodes. The nodes that don't split any further are called leaves (or terminal nodes) and depict the final outcomes of the decision-making process (Figure 3).
2 The software is available at http://www.csie.ntu.edu.tw/~cjlin/libsvm/ 6
http://mc.manuscriptcentral.com/for

Journal of Forecasting

Page 8 of 28

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

For Peer Review

21

22

23

24

25

26

Figure 3. Example of a decision tree.

27

28

29

30

In this paper we use the CART (Classification And Regression Trees) algorithm. The

31

32

CART algorithm creates binary decision trees using the Gini impurity split criterion (Breiman

33

34

et al. 1984). Given the true labels distribution, Gini impurity measures the frequency that a

35

36 37

randomly selected element in a set, may be randomly labeled incorrectly. Trivially, in the case

38 39

that every leaf contains elements of one class, the Gini impurity is zero.

40

41

In a set D of m-sized vectors distributed in L classes, i∈{1,...,L}, the subset of D with

42

43

the elements that belong to the i-th class is denoted D(𝑖). The Gini impurity of D is calculated

44

45

as

46

47

𝐿

(

48 49 50

∑ 𝐺𝑖𝑛𝑖(D) = 1 - 𝑝2𝑖 ,

𝑖=1

(3)

51

|D(𝑖)|

52

where pi is the relative frequency of appearance of class i in D calculated as |D| ,

53

54

(where |D| denotes the cardinality of the set D).

55

56

57

58

59

60

7 http://mc.manuscriptcentral.com/for

Page 9 of 28
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60

For Peer Review

Journal of Forecasting
The CART growing procedure stops when one of the two standard stopping criteria is met: a. Every leaf has elements of only one class or b. Some user specified limit on either the maximum number of splits or the maximum number of leaf nodes is reached.
2.3. Random Forests
Decision trees are easy to interpret but their main drawback is that their generalization ability is low and they perform poorly in out-of-sample data. In other words, they tend to have a low bias but a high variance. Breiman (2001a), introduced the random forests algorithm. It is a learning method that combines the concept of decision trees with the bootstrap aggregating algorithm that is usually called bagging3 (Breiman, 1996). The method avoids the issue of overfitting that may occur in decision trees by combining the result of many decision trees.
When training random forest models, we create many alternative classification trees. For each individual tree, we draw from the initial data set of size 𝑚, a randomly selected with replacement (bootstrapped) sample of the same size. From this procedure, each of the bootstrapped samples contains approximately 2/3 of the initial dataset observations (some of them multiple times) while the rest that is not selected, called the out-of-bag (OOB), is used to test the generalization ability of the trained model. From the initial set of features of cardinality 𝑛, a randomly selected subset of 𝑛 features is used in every tree and at every split. This is done to reduce the dependence of the model to the training set, i.e. to reduce the bias of the model. No observation is a member of every OOB’s in a forest. The response of the RF for any observation of the dataset, is the dominant outcome from all the cases (trees) in which the observation is member of the OOB. The accuracy of the RF model is calculated as the rate of the correctly classified observations.
An important aspect of the random forests methodology is the ability to calculate Variable Importance Measures (VIMs) which can be used to rank the predictors according to
3 from Bootstrapping AGGregatING.
8 http://mc.manuscriptcentral.com/for

Journal of Forecasting

Page 10 of 28

1

2

3

their relevant importance in forecasting the response variable, the target. There are two main

4

5

strains of VIMs: a) the mean decrease in impurity (Gini) VIM and b) the permutation VIM.

6

7 8

The former is mainly used for continuous variables (Breiman et al. 1984) and thus in our

9 10

empirical work we use the latter. The permutation VIM is defined to be the decrease in a

11

12

model’s score (OOB score in our case) when a single feature is randomly shuffled (Breiman,

13

14

2001b). This process breaks the connection between the feature and the target variable; hence

15

16

the decrease in the OOB score is indicative of how much the model relies upon the feature.

17

18

This helps to shed some light in the workings of the so-called black-box of the methodology.

19

20

The ranking of the features is of special interest to policy makers as they can indirectly infer

For Peer Review

21

22

and assess the relative importance of the independent variables.

23

24

25

26

27

2.4. Elastic-Net Logistic Regression (Logit)

28

The logit model is a well-established econometric methodology that is used widely in

29

30 31

the empirical forecasting literature for binary classification. It estimates the probability:

32

33

𝑒𝜷𝑻𝒙

(

34 35

Pr (𝑦 = 1) = 1 + 𝑒𝜷𝑻𝒙

(4)

36

37

Where Pr(𝑦 = 1) is the probability that an event 𝑦 will occur and is assumed to be

38

39

determined by a vector of independent variables 𝑥. In this setting, 𝑦, the dependent binary

40

41

variable (dummy variable), takes the value of 1 or 0. In our case, a value of 0 indicates a

42

43

decrease in the unemployment rate and a value of 1 indicates an increase while β is the vector

44

45

of the estimated coefficients.

46

47 48

The Ridge regression and LASSO (Least Absolute Shrinkage and Selection Operator)

49

50

methodologies are simple techniques that reduce the complexity and possible

51

52

multicollinearity in models with many explanatory variables. Moreover, they prevent

53

54

overfitting that may result from simple linear models, such as the logit, introducing a tolerable

55

56

amount of bias in the model.

57

58

Lasso adds a penalty for non-zero coefficients, but unlike ridge regression which

59

60

penalizes the sum of squared coefficients (L2 penalty), lasso penalizes the sum of their

9 http://mc.manuscriptcentral.com/for

Page 11 of 28

Journal of Forecasting

1

2

3

absolute values (L1 penalty). As a result, employing lasso, many coefficients are exactly

4

5

zeroed. This is never the case in ridge regression. Thus, lasso can be also used as a feature

6

7 8

selection method leading to parsimonious models. The penalty terms are controlled by the

9 10

parameter lambda (λ).

11

12

Elastic net, proposed by Zou and Hastie (2005), is a penalized linear regression model

13

14

that includes both the lasso (L1) and ridge (L2) penalties during training. A parameter α∈(0,

15

16

1], is provided to determine the weight imposed to each of the penalties. As α (alpha) shrinks

17

18

toward 0, elastic net approaches to ridge regression and when α=1 elastic reduces to lasso. In

19

20

our case, the parameters α and λ are optimized within a 5-fold cross validation scheme.

21

For Peer Review

22

23

24 25

2.5. The Dataset

26

The dataset includes the euro-area monthly unemployment rate and 36 additional

27

28

explanatory variables. The frequency is monthly, and the data span the period from 1998:4 to

29

30

2019:9 for a total of 243 observations for each variable. The 36 explanatory variables were

31

32

selected based on economic theory and previous relevant empirical studies (Chen and

33

34

Ranciere, 2019, Groen and Kapetanios, 2016, Kim and Swanson, 2014, Shen, 1996). The data

35

36

for the unemployment rate were obtained from the ECB Statistical Data Warehouse and the

37

38 39

explanatory variables from the ECB Statistical Data Warehouse, the Federal Reserve Bank of

40

41

St. Louis, the Center for Economic Policy Research (CEPR) and Yahoo Finance. Table 1

42

43

summarizes these variables.

44

45

46

47

Table 1. List of explanatory variables used with sources and type of variable.

48

49

Variable

Source

Type

50

Unemployment rate female

ECB Statistical

percent

51

52

Unemployment rate male

53

54

EuroCoin Index

55

56

57 58

WTI Oil Prices (US dollars)

59

60

WTI Oil Prices (US dollars)

Data Warehouse ECB Statistical Data Warehouse Center for Economic Policy Research Federal Reserve Bank of St. Louis Federal Reserve

percent index
average end of period

10 http://mc.manuscriptcentral.com/for

Journal of Forecasting

Page 12 of 28

1

2

3

Bank of St. Louis

4

5

10 Year Euro Bond Rate

Federal Reserve percent

6

Bank of St. Louis

7

Henry Hub Natural Gas Spot Price

Federal Reserve Dollars per million

8

Bank of St. Louis Btu, average

9

Henry Hub Natural Gas Spot Price

Federal Reserve Dollars per million

10

Bank of St. Louis Btu, end of period

11

USD/EUR

ECB Statistical

end of period

12

Data Warehouse

13

AUD/EUR

ECB Statistical

end of period

14

Data Warehouse

15

CAD/EUR

ECB Statistical

end of period

16

Data Warehouse

17

JPY/EUR

ECB Statistical

end of period

18

Data Warehouse

19 20 21 22

GBP/EUR Trade Weighted U.S. Dollar Index: Broad,

ECB Statistical Data Warehouse Federal Reserve

end of period average, Index Jan

23

Goods

Bank of St. Louis 1997=100

24

Trade Weighted U.S. Dollar Index: Broad,

Federal Reserve end of period,

25

Goods

Bank of St. Louis Index Jan

26

1997=100

For Peer Review

27

Trade Weighted U.S. Dollar Index: Major

Federal Reserve average, Index Mar

28

Currencies, Goods

Bank of St. Louis 1973=100

29

Trade Weighted U.S. Dollar Index: Major

Federal Reserve end of period,

30

Currencies, Goods

Bank of St. Louis Index Mar

31

1973=100

32

M1 euro area

Federal Reserve Billions of dollars

33

Bank of St. Louis

34

M3 euro area

35

36

M1 US

37

38

M2 US

39

40 41

M3 US

42

43

MZM US

44

45

Dow Jones

46

Federal Reserve Bank of St. Louis Federal Reserve Bank of St. Louis Federal Reserve Bank of St. Louis Federal Reserve Bank of St. Louis Federal Reserve Bank of St. Louis Yahoo Finance

Billions of dollars Billions of dollars Billions of dollars Billions of dollars Billions of dollars adjusted close, end of period

47

NASDAQ

Yahoo Finance

adjusted close, end

48

of period

49

S&P 500

Yahoo Finance

adjusted close, end

50

of period

51

DAX (German Stock Exchange)

Yahoo Finance

adjusted close, end

52

of period

53

CAC 40 (French Stock Exchange)

Yahoo Finance

adjusted close, end

54

of period

55

International Trade: Exports: Value (goods):

Federal Reserve levels

56

Total for the Euro Area

Bank of St. Louis

57

International Trade: Imports: Value (goods):

Federal Reserve levels

58 59 60

Total for the Euro Area Consumer Price Index: Total All Items for the

Bank of St. Louis Federal Reserve Growth rate

11 http://mc.manuscriptcentral.com/for

Page 13 of 28

Journal of Forecasting

1

2

3

United States

Bank of St. Louis

4

5

Consumer Price Index: Total All Items for the Federal Reserve index

6

United States

Bank of St. Louis

7

Consumer Price Index: All items: Total: Total Federal Reserve Growth rate

8

for the Euro Area

Bank of St. Louis

9

Consumer Price Index: All items: Total: Total Federal Reserve index

10

for the Euro Area

Bank of St. Louis

11

Gold Fixing Price 3:00 P.M. (London time) in Federal Reserve average

12

London Bullion Market, based in U.S. Dollars Bank of St. Louis

13

Gold Fixing Price 3:00 P.M. (London time) in Federal Reserve end of period

14

London Bullion Market, based in U.S. Dollars Bank of St. Louis

15

16

17

18

The target variable (label) is defined by the first difference of the unemployment rate:

19

20

0 when the difference is negative implying that the unemployment rate decreases and 1 when

For Peer Review

21

22

the difference is positive and the unemployment rate rises. In the cases where the

23

24

unemployment rate does not change in two subsequent months and the difference is 0, we

25

26

assigned the label of the previous difference. All variables except for the ones expressing

27

28

percentages are transformed into natural logarithms and the unemployment rate is

29

30 31

transformed in first differences. The dataset was randomly permutated to establish that the

32

33

training set and the out-of-sample set have a comparable ratio of both classes. The

34

35

permutation procedure does not use future information as the whole rows are shuffled.

36

37

We tested the data for stationarity using the ADF test. All data are stationary due to

38

39

the permutation. For the SVM, the data were normalized to the [−1, 1] range.

40

41

We created various data subsets for training and evaluating our models (Table 2):

42

43



Training Set (TS): The optimal parameter values for all models are

44

45

identified in the training step. For the SVM (in all three kernels) and the DT models

46

47

we employ a 5-fold cross-validation procedure. The optimal parameters are the ones

48

49 50

that provide the maximum average accuracy in all five folds. For all models the

51

52

training set included the same 80% of the full dataset, i.e. 195 observations.

53

54



Out-Of-Bag (OOB) set: In the case of the RF, there is no need for

55

56

cross-validation to avoid over-fitting, as the OOB observations are selected randomly

57

58

for each tree in each forest. Out-Of-Bag is composed by the observations that are not

59

60

used when creating the bootstrapped sets for training the trees. For the RF models, the

12 http://mc.manuscriptcentral.com/for

Journal of Forecasting

Page 14 of 28

1

2

3

optimal parameter values are the ones that maximize the accuracy in the OOB set. For

4

5

every tree, the OOB set is not used anywhere in the training step.

6

7 8



Out-Of-Sample (OOS) set: This is 20% of the full dataset, i.e. 48

9

10

observations that were left aside from the training or the selection of the best model

11

12

from each methodology. The OOS set is used to gauge the performance of the

13

14

optimal trained models from each methodology to new data that were never seen by

15

16

the models and eventually detect overfitting.

17

18



Full Dataset (FD): The overall optimal model was identified by

19

20

feeding the top model from each methodology with the initial full dataset

21

For Peer Review

22

(FD=TSOOS).

23

24

25

26 27

Table 2. Model selection procedure

28

Methodology

Model

Optimal parameters

Test

Optimal

29

selection via

overfitting overall model

30 31 32

SVM-Linear SVM-RBF

Training Training

Max average CV-test Max average CV-test

OOS OOS

33

SVM-Polynomial

Training Max average CV-test OOS

Full Dataset

34

Decision Trees

Training Max average CV-test OOS

35

Random Forests

Training Max average OOB

OOS

36

Elastic-Net Logit Estimation Min deviance CV-test OOS

37

38

39

40

41

3. Empirical results

42

43

44 45

3.1. SVM models

46

With the SVM models we proceeded in two steps: first we produced the best

47

48

autoregressive model AR(q) for each kernel (linear, RBF and polynomial) and then we

49

50

augmented it to a structural model by adding the explanatory variables one by one. The

51

52

autoregressive models were created using lagged values of the unemployment rate first

53

54

differences. For the optimization of the hyper-parameters we used a cross-validation

55

56

procedure with a coarse-to-fine grid search scheme. We considered a maximum of 15 lags.

57

58

The model with the highest training set accuracy is chosen as the best AR(q) model for each

59

60

13 http://mc.manuscriptcentral.com/for

Page 15 of 28

Journal of Forecasting

1

2

3

kernel (Figure 4). According to this, the optimal AR(q) models are an AR(4) for the linear

4

5

kernel, an AR(3) for the RBF kernel and an AR(7) for the polynomial kernel. Both the linear

6

7 8

and the RBF kernel based AR models achieved a training set accuracy of 85.64% and the

9 10

polynomial kernel based models a training set accuracy of 84.1%.

11

12

13

14

86

15

16

85

17

18

84

Accuracy %

19

20

83

For Peer Review

21

82

22

23

81

24

25

80

Number of Lags

26 27

79 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15

28

linear

81.5485.1385.1385.6484.6285.1385.1385.1385.1384.6284.1084.1084.1083.5984.10

29 30

rbf

81.5485.1385.6485.6485.6484.6284.6284.6285.1385.1385.1385.1385.1384.1084.10

31

polynomial 79.4981.0381.0381.5483.5982.5684.1083.5983.0883.5983.0882.5683.5983.0881.03

32

33

Figure 4. Training set accuracy of SVM-AR models for the three kernels.

34

35

36

37

In the second step, the explanatory variables were sequentially added, one by one, in

38

39

the best autoregressive models identified in the previous step for each kernel. The training set

40

41 42

accuracy of these augmented models is reported in Table 3. The best models, for all three

43

44

kernels, are the ones that are augmented with the DAX. The linear and the RBF4 kernel

45

46

models reached an 86.67% accuracy and the polynomial 85.13%. We proceeded by adding

47

48

sequentially a second explanatory variable, but this did not increase the model accuracy in

49

50

any case.

51

52

53

54

55

56

57

58

4 For the RBF kernel the augmented models with WTI prices (end of period) and M3 for eurozone also

59

achieve an accuracy of 86.67% but the train accuracy (average train cross-validation) is higher for the

60

model with the DAX variable and is selected as the best augmented RBF model.

14 http://mc.manuscriptcentral.com/for

Journal of Forecasting

Page 16 of 28

1

2

3

4

5

Table 3. Training set directional forecasting accuracy of the explanatory variables: linear,

6

RBF and polynomial kernels.

7

8

Variable

9

10

Best AR

Linear 85,64%

RBF 85,64%

Polynomial 84,10%

11

Unemployment rate eurozone female

84,62%

85,13%

83,08%

12

Unemployment rate eurozone male

84,10%

85,13%

84,10%

13

10y Euro Bond rate

14

15

CPI EU growth rate

85,64% 85,64%

85,13% 86,15%

83,59% 82,56%

16

CPI EU index

85,13%

85,13%

84,10%

17

CPI US growth rate

85,13%

84,62%

83,08%

18

CPI US index

19

20

Trade Weighted US goods, average

85,13% 85,64%

85,13% 85,64%

83,08% 83,59%

For Peer Review

21

Trade Weighted US goods, end of period

85,64%

85,13%

83,08%

22

Trade Weighted US currencies, average

84,62%

84,10%

83,59%

23

Trade Weighted US currencies, end of period

85,13%

84,10%

82,05%

24

25

WTI average LN

85,64%

86,15%

83,59%

26

WTI end of period LN

85,64%

86,67% * 84,10%

27

Henry Hub natural gas spot prices, average LN

85,64%

83,08%

83,08%

28

Henry Hub natural gas spot prices, end of period LN 86,15%

83,08%

82,56%

29

30

M1 EU LN

85,13%

85,64%

83,08%

31

M3 EU LN

85,13%

86,67% * 84,62%

32

M1 US LN

85,64%

85,13%

84,10%

33

M2 US LN

34

35

M3 US LN

85,13% 85,13%

85,64% 85,64%

83,59% 83,59%

36

MZM US LN

85,13%

85,64%

84,10%

37

Exports EU levels LN

85,13%

84,62%

83,59%

38

Imports EU levels LN

39

40

Gold prices, average LN

85,13% 85,13%

85,64% 86,15%

84,10% 83,59%

41

Gold prices, end of period LN

85,64%

86,15%

83,59%

42

Eurocoin index

84,62%

83,59%

84,62%

43

USD/EUR

44

45

AUD/EUR

85,64% 84,10%

84,62% 86,15%

81,54% 84,10%

46

CAD/EUR

83,08%

85,13%

83,59%

47

JPY/EUR

84,62%

82,05%

81,54%

48

GBP/EUR

49

50

Dow Jones LN

85,13% 86,15%

85,13% 85,64%

83,59% 84,10%

51

Nasdaq LN

85,13%

85,64%

83,59%

52

S&P 500 LN

85,13%

86,15%

83,59%

53

DAX LN

86,67% * 86,67% * 85,13% *

54

55

CAC40 LN

86,15%

86,15%

83,59%

56

An asterisk (*) denotes the best augmented model for each kernel.

57

58

59

60

15 http://mc.manuscriptcentral.com/for

Page 17 of 28

Journal of Forecasting

1

2

3

4

5

6

3.2. Decision Trees models

7

For the decision trees we trained the model using all the explanatory variables. We

8

9

trained various models with the number of decision nodes ranging from 1 to 156 and the leaf

10

11

size ranging from 1 to 50 using cross validation and grid search for the hyper-parameter

12

13

optimization. The optimal forecasting model has 2 decision nodes and 3 leaf nodes. The first

14

15 16

node (parent node) is defined by the criterion “the first difference of the unemployment rate at

17 18

lag 2 is less than 0.015”. If the criterion is not satisfied, the model forecasts that the

19

20

unemployment rate will increase (label=1). Otherwise, the tree continues in the second node,

For Peer Review

21

22

where the criterion is “the Nasdaq is less than 7.64”. If this criterion is satisfied the model

23

24

forecasts that the unemployment rate will rise (label=1); in the opposite case the model

25

26

forecasts that the unemployment rate will decrease (label=0) (Figure 5). The training set

27

28

forecasting accuracy of the best decision tree model is 83.08%.

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

52

53

Figure 5. Visualization of the trained decision tree model for the directional forecasting of the

54

euro area unemployment rate. When label=1, unemployment increases and when label=0, it

55

decreases.

56

57

58

59

60

16 http://mc.manuscriptcentral.com/for

Journal of Forecasting

Page 18 of 28

1

2

3

3.3. Random Forest models

4

5

As we mention earlier, our training sample includes 195 observations. With these we

6

7

created 950 alternative forests that include trees ranging from 50 to 1000. Each tree in these

8

9

models uses a bootstrapped sample (sampling with replacement) of the initial size of 195

10

11

observations. At every node a different, randomly selected, subset of explanatory variables is

12

13

used. The cardinality οf this subset is 𝑛 as proposed by Geurts et al. (2006), where 𝑛 is the

14

15

number of explanatory variables (features); in our sample, 𝑁=51 (36 explanatory variables

16

17 18

and 15 lags of the unemployment rate first differences). This is done to reduce the

19

20

dependence of the model to the training data, which is the basic cause of overfitting (low bias

For Peer Review

21

22

and high variance).

23

24

We trained our models and used grid search for the hyper-parameter optimization.

25

26

Namely, we trained models with maximum number of splits ranging from 1 to 5 and number

27

28

of trees ranging from 50 to 1000. The OOB accuracy was used to select the best model. The

29

30

optimal forest includes 88 trees and the maximum number of splits is equal to 5. The

31

32

corresponding training set accuracy (OOB accuracy) is 85.64%.

33

34

35

36

37 38

3.4. Elastic-Net Logistic Regression (Logit) model

39

We optimized the parameters α and λ in a 5-fold cross-validation scheme. The model

40

41

with minimum binomial deviance is selected as optimal (Zou and Hastie, 2005, Hastie et. al,

42

43

2009). Deviance is defined as the difference of likelihoods between the fitted model and the

44

45

saturated model (perfect model). The optimal model is an elastic net regularization logit

46

47

model with α=0.7. The selected variables (non-zero fitted coefficients) are shown in Table 4.

48

49 50

The training set accuracy of this model is 86.67%.

51

52

53 54

Table 4. Selected variables with non-zero fitted coefficients

55

56

1 Lag 1 of unemployment rate (first differences)

57

2 Lag 2 of unemployment rate (first differences)

58

3 Lag 6 of unemployment rate (first differences)

59

4 Unemployment rate female

60

17 http://mc.manuscriptcentral.com/for

Page 19 of 28

Journal of Forecasting

1

2

3 4

5 10 year Euro Bond rate

5

6 Eurocoin index

6

7 CAD/EUR

7

8 Nasdaq

8 9

9 CAC40

10

11

12

3.5. Overall Best Model

13

14

Thus far, we have identified the best model from each methodology (SVM, DT, RF,

15

16

Elastic-net logit) using the same training set. In this section, we feed these models with the

17

18

full dataset and we select the best overall model. This is the model that achieves the highest

19

20

accuracy in directionally forecasting the monthly unemployment rate in euro-area. We use the

For Peer Review

21

22

full-dataset accuracy since we used different training schemes (OOB for the random forest,

23

24

cross-validation for the SVM, the DT and the Elastic-net logit), and it would be wrong to

25

26

compare the performance of the created models using the training set accuracy. We also

27

28

check the performance of the models in the out-of-sample part only to evaluate the

29

30 31

forecasting performance on data that was not used during the training process i.e. the

32

33

possibility of having low bias (high accuracy in training set) and high variance (low

34

35

generalization ability). The results are summarized in Figure 6.

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

52

53

54

55

56

57

58

59

60

18 http://mc.manuscriptcentral.com/for

Journal of Forecasting

Page 20 of 28

1

2

3

4

5

90.00

full-dataset out-of-sample training set

6

7

88.00

8

86.00

9

10

84.00

Αccuracy %

11

12

82.00

13

80.00

14

15

78.00

16 17

76.00

18

74.00

19

20 21

72.00

SVM linear

SVM rbf

SVM polynomial

DT

RF

Elastic-Net Logit

For Peer Review

22

full-dataset

86.01

85.19

85.19

84.36

88.48

86.01

23

out-of-sample 83.33

81.25

77.08

85.42

85.42

83.33

24

training set

86.67

86.67

85.13

83.08

85.64

86.67

25

26

27

Figure 6. Full-dataset, out-of-sample and training set directional forecasting accuracy of euro

28

area unemployment rate: decision trees, random forest SVM (linear, RBF and polynomial)

29

and Elastic-net logit models.

30

31

32

33

According to the results, the SVM coupled with the linear kernel reached a full-

34

35

dataset accuracy of 86.01% and an out-of-sample accuracy of 83.33%. The SVM coupled

36

37

with the RBF kernel reached a full-dataset accuracy of 85.18% and an out-of-sample accuracy

38

39

of 81.25%. The SVM model with the polynomial kernel reached a full-dataset accuracy of

40

41

85.19% and an out-of-sample accuracy of 77.08%. The best SVM model is the one coupled

42

43

with the linear kernel. The decision tree model reached a full-dataset accuracy of 84.36% and

44

45 46

an out of sample accuracy of 85.42%, while the random forest model has a full-dataset

47

48

accuracy of 88.48% and out-of-sample 85.42%. Finally, the elastic-net logit model achieved a

49

50

full-dataset accuracy of 86.01% and an out-of-sample accuracy of 83.33%. According to these

51

52

results, the best overall directional forecasting model for the euro area unemployment rate is

53

54

the random forest model.

55

56

57

58

59

60

19 http://mc.manuscriptcentral.com/for

Page 21 of 28

Journal of Forecasting

1

2

3

3.6. Variable Importance Measure results

4

5

As mentioned in the empirical part, we used the permutation VIM to identify and

6

7

interpret the forecasting ability of the explanatory variables, for the random forest model. The

8

9

permutation VIM calculates the loss in the OOB score after the permutation of each variable,

10

11

showing the dependence of the model on each variable. In Table 5 we present the ranking of

12

13

the variables according to permutation VIM. According to this ranking, the 10 most important

14

15

variables are: the first 4 lags of the euro area unemployment rate, the Eurocoin index, 4

16

17

financial indices (S&P500, Nasdaq, Dow Jones and CAC40) and the WTI prices.

18

19

20

For Peer Review

21 22

Table 5. Permutation feature importance with random forest ensemble method.

23

24

Variable

Permutation VIM

25

lag1

0,5310

26

lag2

27 28

S&P 500

29

Eurocoin index

0,4889 0,4322 0,3601

30

lag3

0,3352

31

Nasdaq

32 33

Dow Jones

34

lag4

0,3347 0,3324 0,3162

35

WTI average

0,3129

36

CAC40

0,2846

37 38

Trade Weighted US currencies average

0,2758

39

Trade Weighted US goods end of period

0,2648

40

lag5

0,2626

41

DAX

42 43

USD/EUR

44

lag6

0,2612 0,2486 0,2299

45

CAD/EUR

0,2211

46

AUD/EUR

47 48

M3 US

49

Gold prices average

0,2200 0,2137 0,2122

50

10y Euro Bond rate

0,2080

51

lag7

52 53

CPI EU growth rate

54

MZM US

0,2004 0,1991 0,1957

55

Trade Weighted US currencies end of period

0,1851

56

lag8

57 58

CPI US index

59

M1 EU

0,1796 0,1714 0,1703

60

Trade Weighted US goods average

0,1638

20 http://mc.manuscriptcentral.com/for

Journal of Forecasting

Page 22 of 28

1

2

3 4

lag9

5

CPI EU index

0,1578 0,1438

6

M1 US

0,1434

7

WTI end of period

8 9

Gold prices end of period

10

Unemployment rate eurozone female

0,1397 0,1379 0,1349

11

M2 US

0,1060

12

lag12

13 14

Imports EU levels

15

CPI US growth rate

0,1060 0,1037 0,1019

16

GBP/EUR

0,0912

17

lag15

18 19

lag14

20

lag11

0,0676 0,0582 0,0534

For Peer Review

21

Unemployment rate eurozone male

0,0255

22

JPY/EUR

23 24

Exports EU levels

25

M3 EU

0,0171 0,0000 0,0000

26

lag10

-0,0038

27

Henry Hub natural gas spot prices average

-0,0455

28 29

Henry Hub natural gas spot prices end of period

-0,1494

30

lag13

-0,1716

31

32

33

Lags seem to play an important role in forecasting the unemployment rate. It seems

34

35

that unemployment follows short-run trends. In the decision tree model the lag2 is selected in

36

37

one out of the 2 decision nodes. In the case of the random forests, that as we have seen

38

39

achieves the best accuracy over all the tested methodologies, the permutation VIM assigns the

40

41

highest scores to lag1 and lag2 of the unemployment rate.

42

43 44

Stock market indices seem to play an important role in euro-area unemployment rate

45 46

forecasting. Stock prices reflect the present value of the market’s expectations of future

47

48

corporate profits. Thus, it is expected that either directly or indirectly there may be a positive

49

50

link between stock indices, output and employment. Economic theory suggests that the

51

52

expectation of an increased demand in the foreseeable future will be followed by a rise in

53

54

supply. This increase in supply can be achieved through new investment and/or increased

55

56

usage of the existing production capacity. In both cases, it is expected to observe an increase

57

58

in the demand for the two factors of production: capital and labor. As a result, the

59

60

unemployment rate in the near future is expected to fall.

21 http://mc.manuscriptcentral.com/for

Page 23 of 28

Journal of Forecasting

1

2

3

In the SVM models, the DAX provides the highest forecasting accuracy for all SVM

4

5

kernels and the Nasdaq is selected in one of the two decision nodes in the decision trees

6

7 8

model. The permutation VIM for the random forests reports 4 financial indices among the 10

9 10

most important variables: the S&P500, the Nasdaq, the Dow Jones (related to the U.S. stock

11

12

market), and the CAC40 (related to the French stock market). Similar findings are reported in

13

14

several papers in the literature. Farmer in 2010 and 2015 shows that the stock market contains

15

16

significant information about future unemployment. Pan (2018) found a particularly strong

17

18

and one-way causal direction from stock prices to the unemployment rate in G7 countries and

19

20

a strong bilateral causal relationship between stock prices and unemployment for other

For Peer Review

21

22

advanced countries. Sibande et al., 2019 use DCC-MGARCH tests, to analyze time-varying

23

24

causality between stock market returns and unemployment in the UK.

25

26

27

28 29

4. Conclusion

30

The unemployment, obviously, has many serious social, political and personal

31

32

negative implications as the right to work is one of the basic rights in a modern democracy.

33

34

Additionally, from the economics standpoint, unemployment creates significant

35

36

inefficiencies. The GDP is sub-optimal, as part of the human capital is not used in production.

37

38 39

Moreover, unemployment imposes significant costs to governments in terms of the associated

40 41

benefits, re-training programs and the mechanisms that are put in place for tracking,

42

43

monitoring and minimizing the people that are out of jobs.

44

45

As a result, accurately measuring, reducing and controlling the unemployment rate is

46

47

one of the main goals of modern governments. Policy makers from both the government and

48

49

the central bank, usually take into account as one the variables of main concern the

50

51

unemployment rate when they design and implement fiscal and monetary policy.

52

53

In this study, we attempted to directionally forecast, the euro-area unemployment

54

55

rate. To the best of our knowledge, this is the first time that the EU-area over-all

56

57

unemployment rate is forecasted. In the empirical section, we employed decision trees,

58

59 60

random forests and SVM models with the linear, the RBF and the polynomial kernels. These

22 http://mc.manuscriptcentral.com/for

Journal of Forecasting

Page 24 of 28

1

2

3

methodologies were further compared to a logit forecasting model under an elastic-net

4

5

framework from the area of econometrics. The direction of the euro-area unemployment rate

6

7 8

is the dependent variable and 36 explanatory variables spanning the period from 1998:4 to

9 10

2019:9 comprised our dataset. We train all models using 80% of the dataset and the remaining

11

12

20% was used as the out-of-sample data to detect possible overfitting and evaluate the

13

14

generalization ability of our models to new and unknown data.

15

16

Overall, the best accuracy was achieved by the random forest model: 88.48% on the

17

18

full-dataset and 85.42% on the out-of-sample. According to the permutation VIM the most

19

20

important of the explanatory variables are: the first 4 lags of the unemployment rate, 4 out of

For Peer Review

21

22

5 tested financial indices (S&P500, Nasdaq, Dow Jones and the CAC40), the Eurocoin index

23

24

and the WTI prices. Stock markets seem to play an important role in forecasting

25

26 27

unemployment in all employed models. In all SVM kernels, the use of the DAX provides a

28 29

higher forecasting accuracy, the Nasdaq is selected in the decision trees models and 4

30

31

financial indices are included in the best random forest model. Thus, we find strong evidence

32

33

that the use of financial indices seems to play an important role in the euro-area

34

35

unemployment forecasting.

36

37

38

39

Acknowledgements: «This research is co-financed by Greece and the European

40

41

Union (European Social Fund- ESF) through the Operational Programme «Human Resources

42

43

Development, Education and Lifelong Learning» in the context of the project “Strengthening

44

45 46

Human Resources Research Potential via Doctorate Research” (MIS-5000432), implemented

47

48

by the State Scholarships Foundation (ΙΚΥ)»

49

50

51

52

53

54

55

56

57

58

59

60

23 http://mc.manuscriptcentral.com/for

Page 25 of 28

Journal of Forecasting

1

2

3 4

References

5

6

7

 Barot, B. (2004), How accurate are the Swedish forecasters on GDB-Growth, CPI-

8

9

inflation and unemployment? (1993 - 2001). Brussels Economic Review. 47. 249-278.

10

11

 Breiman, L. (1996). Bagging predictors. Machine Learning, 24(2), pp.123-140.

12

13 14

 Breiman, L. (2001)a. Random forests. Machine Learning, 45 pp.5-32

15 16

 Breiman, L. (2001)b. Statistical Modeling: The Two Cultures (with comments and a

17

18

rejoinder by the author). Statistical Science, 16(3), pp.199-231.

19

20

 Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. (1984). Classification and

For Peer Review

21

22

regression trees. Wadsworth. Inc. Monterey, California, USA.

23

24

 Brunner, K., Cukierman, A. and Meltzer, A. (1980). Stagflation, persistent

25

26

unemployment and the permanence of economic shocks. Journal of Monetary

27

28

Economics, 6(4), pp.467-492.

29

30 31

 Chang, C. and Lin, C. (2011). LIBSVM. ACM Transactions on Intelligent Systems

32

33

and Technology, 2(3), pp.1-27.

34

35

 Chen, S. and Ranciere, R. (2019). Financial information and macroeconomic

36

37

forecasts. International Journal of Forecasting, 35(3), pp.1160-1174.

38

39

 Claveria, O. (2019). Forecasting the unemployment rate using the degree of agreement

40

41

in consumer unemployment expectations. Journal for Labour Market Research, 53(1).

42

43 44

 Cook, T. and Smalter Hall, A. (2017). Macroeconomic Indicator Forecasting with

45 46

Deep Neural Networks. The Federal Reserve Bank of Kansas City Research Working

47

48

Papers,.

49

50

 Cortes, C. and Vapnik, V. (1995). Support-vector networks. Machine Learning, 20(3),

51

52

pp.273-297.

53

54

 D’Amuri, F. and Marcucci, J. (2010). 'Google It!' Forecasting the US Unemployment

55

56

Rate with A Google Job Search Index. SSRN Electronic Journal,.

57

58

59

60

24 http://mc.manuscriptcentral.com/for

Journal of Forecasting

Page 26 of 28

1

2

3

 Dobre, I. and Alexandru, A. A. (2008). Modelling unemployment rate using Box–

4

5

Jenkins procedure. Journal of Applied Quantitative Methods, 3(2), 156–166.

6

7 8

 Dumičić, K., Časni, A., Žmuk, B. (2015). Forecasting Unemployment Rate in Selected

9

10

European Countries Using Smoothing Methods. World Academy of Science,

11

12

Engineering and Technology: International Journal of Social Education, Economics

13

14

and Management Engineering, 9(4), 867 - 872.

15

16

 Farmer, R. (2010). How to reduce unemployment: A new policy proposal. Journal of

17

18

Monetary Economics, 57(5), pp.557-572.

19

20

 Farmer, R. (2015). The Stock Market Crash Really Did Cause the Great

21

For Peer Review

22 23

Recession. Oxford Bulletin of Economics and Statistics, 77(5), pp.617-633.

24 25

 Floros, C. (2005). Forecasting the UK unemployment rate: Model comparisons.

26

27

International Journal of Applied Econometrics and Quantitative Studies, 2(4), 57–72.

28

29

 Geurts, P., Ernst, D. and Wehenkel, L. (2006). Extremely randomized trees. Mach

30

31

Learn, 63, pp. 3–42

32

33

 Gogas, P., Papadimitriou, T. and Sofianos, E. (2019). Money Neutrality, Monetary

34

35

Aggregates and Machine Learning. Algorithms, 12(7), p.137.

36

37 38

 Groen, J. and Kapetanios, G. (2016). Revisiting useful approaches to data-rich

39

40

macroeconomic forecasting. Computational Statistics & Data Analysis, 100, pp.221-

41

42

239.

43

44

 Grubb, D., Jackman, R. and Layard, R. (1982). Causes of the Current Stagflation. The

45

46

Review of Economic Studies, 49(5), p.707.

47

48

 Hastie, T., Tibshirani, R., & Friedman, J. (2009). The elements of statistical learning:

49

50

data mining, inference, and prediction. Springer Science & Business Media.

51

52 53

 Katris, C. (2019). Prediction of Unemployment Rates with Time Series and Machine

54 55

Learning Techniques. Computational Economics, 55(2), pp.673-706.

56

57

58

59

60

25 http://mc.manuscriptcentral.com/for

Page 27 of 28
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60

For Peer Review

Journal of Forecasting
 Kim, H. and Swanson, N. (2014). Forecasting financial and macroeconomic variables using data reduction methods: New empirical evidence. Journal of Econometrics, 178, pp.352-367.
 Kreiner, A. and Duca, J. (2019). Can machine learning on economic data better forecast the unemployment rate? Applied Economics Letters, pp.1-4.
 Kurita T. (2010). A Forecasting Model for Japan's Unemployment Rate. Eurasian Journal of Business and Economics, 3 (5), 127-134
 Mladenovic, J., Ilic, I., & Kostic, Z. (2017). Modeling the unemployment rate at the eu level by using Box–Jenkins methodology. KnE Social Sciences, 1(2), 1–13.
 Muth, J. (1961). Rational Expectations and the Theory of Price Movements. Econometrica, 29(3), p.315.
 Pan, W. (2018). Does the stock market really cause unemployment? A cross-country analysis. The North American Journal of Economics and Finance, 44, pp.34-43.
 Sermpinis, G., Stasinakis, C., Theofilatos, K. and Karathanasopoulos, A. (2014). Inflation and Unemployment Forecasting with Genetic Support Vector Regression. Journal of Forecasting, 33(6), pp.471-487.
 Shen, C. (1996). Forecasting macroeconomic variables using data of different periodicities. International Journal of Forecasting, 12(2), pp.269-282.
 Sibande, X., Gupta, R. and Wohar, M. (2019). Time-varying causal relationship between stock market and unemployment in the United Kingdom: Historical evidence from 1855 to 2017. Journal of Multinational Financial Management, 49, pp.81-88.
 Stasinakis, C., Sermpinis, G., Theofilatos, K. and Karathanasopoulos, A. (2014). Forecasting US Unemployment with Radial Basis Neural Networks, Kalman Filters and Support Vector Regressions. Computational Economics, 47(4), pp.569-587.
 Xu, W., Li, Z., Cheng, C. and Zheng, T. (2013). Data mining for unemployment rate prediction using search engine query data. Service Oriented Computing and Applications, 7(1), pp.33-42.
26 http://mc.manuscriptcentral.com/for

Journal of Forecasting

Page 28 of 28

1

2

3

 Zou, H. and Hastie, T. (2005). Regularization and variable selection via the elastic

4

5

net. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(2),

6

7 8

pp.301-320.

9

10

11

12

Data Availability Statement

13

14

15

The data that support the findings of this study are available from the corresponding

16

17

author upon reasonable request.

18

19

20

For Peer Review

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

52

53

54

55

56

57

58

59

60

27 http://mc.manuscriptcentral.com/for

View publication stats

